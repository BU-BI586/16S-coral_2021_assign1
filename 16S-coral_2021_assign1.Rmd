---
title: "Grey Patch Disease in Coral and 16S Analysis of Bacteria"
author: "Susritha Kopparapu, Isabel Novick, Corinne Vietorisz"
date: "2/28/2021"
output: html_document
---
## Introduction

Coral reefs are increasingly threatened by climate change, thus the need to identify the diversity of the microbial communities of coral before they die off is becoming more urgent in order to understand the roles their microbial communities play in coral health. Microbial communities may affect host coral fitness, and coral disease may influence the integrity of their microbiome, exacerbating the negative effects. We sought to examine how coral microbial communities change after outbreaks of grey-patch disease. 

Our data is from Sweet et al. (2019), "Compositional homogeneity in the pathobiome of a new, slow-spreading coral disease." We identified and examined the bacterial communities on healthy and diseased corals using 16S sequencing and DNA barcoding. A reanalysis of the Sweet et al. (2019) data was performed using the dada2 pipeline in R followed by the assigning of microbial taxonomy with the Silva 16S database. We found that diseased colonies have a higher abundance of cyanobacteria than healthy colonies, in accordance with the findings of Sweet et al (2019). We additionally found that healthy colonies had higher abundances of gammaproteobacteria compared to diseased colonies. 


## Setup

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

#### R Version
R version 4.0.2 was used for this analysis. 

#### Packages
The following packages were used to clean, analyze, and visualize data. Packages were installed with [Bioconductor](https://bioconductor.org/biocLite.R) version 3.12 using the vegan package. 
```{r, message=FALSE}
library(dada2);         #Version 1.18.0
library(ShortRead);     #Version 1.48.0
library(ggplot2);       #Version 3.3.3
library(phyloseq);      #Version 1.34.0
```

```{r, echo=FALSE, eval=FALSE}
packageVersion("dada2"); citation("dada2")
packageVersion("ShortRead")
packageVersion("ggplot2")
packageVersion("phyloseq")
```

#### Data Retrieval
[Sweet et. al 2019](https://microbiomejournal.biomedcentral.com/articles/10.1186/s40168-019-0759-6) collected samples from healthy and diseased *Porites lobata*, as well as the water column of the Luminao reef flat in Guam (in 2016). Their data that was reanalyzed here was retrieved from [NCBI](https://www.ncbi.nlm.nih.gov/sra?LinkName=bioproject_sra_all&from_uid=448821).

Samples diseased 13, healthy 11, healthy 4, diseased 14, water column 2 were selected. All files in the directory were put in the variable fns. 
```{r, results=FALSE}
path <- "/projectnb/bi594/skoppara/Assignment1"
fns <- list.files(path)
fns
```

## Trimming and Filtering Data
Filter through all the files and find only the fastq files. Sort them so that the reads are in the same order. 
```{r}
fastqs <- fns[grepl(".fastq$", fns)]
fastqs <- sort(fastqs) 
fastqs
```

Split and store each of the files names as sample names without the .fastq file extension. Store these in another variable.
```{r}
sample.names <- sapply(strsplit(fastqs, ".fastq"), `[`, 1) 
#sample.names
fnFs <- file.path(path, fastqs)
#fnFs

```

## Sequence Verification
*code adapted from [Nicola Kriefall](https://github.com/Nicfall/moorea_holobiont/blob/master/mr_16S/mr16s.R)*

The forward and reverse primers as defined from the Sweet et. al paper are shown below.
```{r}
FWD <- "TATGGTAATTGTCTCCTACTTRRSGCAGCAG"  
REV <- "AGTCAGTCAGCCGGACTACNVGGGTWTCTAAT"
```

The below function takes in a character vector primer and creates all the orientations of that sequence. The vector is converted to a DNA String object using the Biostrings package. The DNA string object can be used to evaluate the complement of the sequences. All orients are converted back to character vectors upon return. 
```{r}
allOrients <- function(primer) {
  require(Biostrings)
  dna <- DNAString(primer)  
  orients <- c(Forward = dna, Complement = complement(dna), Reverse = reverse(dna), 
               RevComp = reverseComplement(dna))
  return(sapply(orients, toString))  
}
```

The forward and reverse orients are as follows. 
```{r, echo=FALSE}
FWD.orients <- allOrients(FWD)
REV.orients <- allOrients(REV)
FWD.orients
REV.orients
```


```{r}
fnFs.filtN <- file.path(path, "filtN", basename(fnFs))
filterAndTrim(fnFs, fnFs.filtN, maxN = 0, multithread = TRUE)
```

The below function takes in a primer sequence and a read. It counts the number of reads in which the primer is found and returns this value. 
```{r, results=FALSE}
primerHits <- function(primer, fn) {
  nhits <- vcountPattern(primer, sread(readFastq(fn)), fixed = FALSE)
  return(sum(nhits > 0))
}
primerHits
```

The below tables show that the primers were removed from the sequences already. No left trim is necessary when filtering the actual sequences. 
```{r}
rbind(FWD.ForwardReads = sapply(FWD.orients, primerHits, fn = fnFs.filtN[[3]]), 
      FWD.ReverseReads = sapply(FWD.orients, primerHits, fn = fnFs.filtN[[3]]), 
      REV.ForwardReads = sapply(REV.orients, primerHits, fn = fnFs.filtN[[3]]), 
      REV.ReverseReads = sapply(REV.orients, primerHits, fn = fnFs.filtN[[3]]))
```



#### Visualize Raw Data

The quality plot below shows that the quality stays above or at 20 for the whole length of the sequence, however, around 200 quality does start to drop to 20. For this reason, the truncation length was chosen to be 200. 
```{r}
plotQualityProfile(fnFs[c(1,2,3,4,5,6,7,8,9)])
```

A directory is made for the filtered files and filenames using the following code. 
```{r, eval=FALSE}
filt_path <- file.path(path, "trimmed")
if(!file_test("-d", filt_path)) dir.create(filt_path)
filtFs <- file.path(filt_path, paste0(sample.names, "_F_filt.fastq.gz"))
```

The truncation length was defined to 200. No Ns are allowed and expected errors allowed was 1. All other settings were set to the defaults. 
```{r, eval=FALSE}
out <- filterAndTrim(fnFs, filtFs, truncLen= 200,
                     maxN=0, 
                     maxEE=1, 
                     truncQ=2, 
                     rm.phix=TRUE, 
                     compress=TRUE, 
                     multithread=FALSE) 
```

```{r,echo=FALSE, eval=FALSE}
head(out)
```


## Error Rates
```{r,eval=FALSE}
setDadaOpt(MAX_CONSIST=30) #increase number of cycles to allow convergence
errF <- learnErrors(filtFs, multithread=TRUE)
#Maximum cycles was set to 30, but Convergence was found after 4 rounds

#sanity check: visualize estimated error rates
#error rates should decline with increasing qual score
#red line is based on definition of quality score alone
#black line is estimated error rate after convergence
#dots are observed error rate for each quality score

plotErrors(errF, nominalQ=TRUE) 
```


## Dereplicate Reads
```{r,eval=FALSE}
#Dereplication combines all identical sequencing reads into “unique sequences” with a corresponding “abundance”: the number of reads with that unique sequence. 
#Dereplication substantially reduces computation time by eliminating redundant comparisons.
#DADA2 retains a summary of the quality information associated with each unique sequence. The consensus quality profile of a unique sequence is the average of the positional qualities from the dereplicated reads. These quality profiles inform the error model of the subsequent denoising step, significantly increasing DADA2’s accuracy.
derepFs <- derepFastq(filtFs, verbose=TRUE)
# Name the derep-class objects by the sample names
names(derepFs) <- sample.names
```


## Infer Sequence Variants
Band size 16 for 16S
```{r,eval=FALSE}
#Must change some of the DADA options b/c original program optomized for ribosomal data, not ITS - from github, "We currently recommend BAND_SIZE=32 for ITS data." leave as default for 16S/18S
setDadaOpt(BAND_SIZE=16)
dadaFs <- dada(derepFs, err=errF, multithread=TRUE)

#now, look at the dada class objects by sample
#will tell how many 'real' variants in unique input seqs
#By default, the dada function processes each sample independently, but pooled processing is available with pool=TRUE and that may give better results for low sampling depths at the cost of increased computation time. See our discussion about pooling samples for sample inference. 
dadaFs[[1]]
dadaFs[[5]]

#construct sequence table
seqtab <- makeSequenceTable(dadaFs)
head(seqtab)
```


## Remove Chimeras
```{r,eval=FALSE}
seqtab.nochim <- removeBimeraDenovo(seqtab, method="consensus", multithread=TRUE, verbose=TRUE)
dim(seqtab.nochim)
# Identified 1 bimeras out of 117 input sequences.

sum(seqtab.nochim)/sum(seqtab)
#The fraction of chimeras varies based on factors including experimental procedures and sample complexity, 
#Most of your reads should remain after chimera removal (it is not uncommon for a majority of sequence variants to be removed though)
#For our sample, this ratio was 0.9998201, there was only 1 bimera

write.csv(seqtab,file="16s_seqtab.csv")
write.csv(seqtab.nochim,file="16s_nochim.csv")
```


## Track Read Stats
```{r,eval=FALSE}
getN <- function(x) sum(getUniques(x))
track <- cbind(out, sapply(dadaFs, getN), sapply(dadaFs, getN), rowSums(seqtab), rowSums(seqtab.nochim))
colnames(track) <- c("input", "filtered", "denoised", "merged", "tabled", "nonchim")
rownames(track) <- sample.names
head(track)
tail(track)

write.csv(track,file="ReadFilterStats_AllData_final.csv",row.names=TRUE,quote=FALSE)

```


## Assign Taxonomy
```{r, eval=FALSE}
taxa <- assignTaxonomy(seqtab.nochim, "GTDB_bac-arc_ssu_r86.fa", minBoot=5,multithread=TRUE,tryRC=TRUE,outputBootstraps=FALSE)
#minboot should be higher
#Obtain a csv file for the taxonomy so that it's easier to map the sequences for the heatmap.
write.csv(taxa, file="taxa.csv",row.name=TRUE,quote=FALSE)
unname(head(taxa, 30))
unname(taxa)
```

## Visualize Data

```{r,echo=FALSE}
setwd("/projectnb/bi594/skoppara/Assignment1/")

```

```{r}
seqtab.nochim <- readRDS("final_seqtab_nochim.rds")
taxa <- readRDS("final_taxa_blastCorrected.rds")
#head(taxa)
samdf<-read.csv("variabletable.csv")
head(samdf)
#head(seqtab.nochim)
#head(taxa)
rownames(samdf) <- samdf$sample
```

```{r,eval=TRUE}
ps <- phyloseq(otu_table(seqtab.nochim, taxa_are_rows=FALSE), 
               sample_data(samdf), 
               tax_table(taxa))
ps
```


```{r, echo=FALSE}
ids<-taxa_names(ps)
ids <- paste0("sq",seq(1, length(colnames(seqtab.nochim))))
colnames(seqtab.nochim) <- ids

#Bar-plots
top90 <- names(sort(taxa_sums(ps), decreasing=TRUE))[1:90]
ps.top90 <- transform_sample_counts(ps, function(OTU) OTU/sum(OTU))
ps.top90 <- prune_taxa(top90, ps.top90)

plot_bar(ps.top90, x="Sample", fill="Class") 
```

```{r, echo=FALSE, fig.width=10, fig.height = 8}
#visusalize via counts rather than abundances:
plot_bar(ps, x = "sample", fill= "Class", labels=c('Diseased1', "Healthy1", "Healthy2", "Diseased 2", "Water column")) #+ facet_wrap("tank")
#
```

```{r, echo=FALSE}
#Obtain a csv file for the phyloseq data. Will give you the abundances for each sample and class. Useful for constructing the heatmap. Also, enables you to use ggplot, and construct more aesthetically pleasing plot.
psz <- psmelt(ps.top90)
write.csv(psz, file="Phyloseqoutputfinal.csv")
p <- ggplot(psz, aes(x=Sample, y=Abundance, fill=Class))
p + geom_bar(stat="identity", colour="black")
```


## Conclusion
insert conclusion here

